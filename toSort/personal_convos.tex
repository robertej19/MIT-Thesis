\section{CLAS12 DC Recon}
    We got this information from Andrey Kim, a postdoc at UConn.
    He’s not a DC expert so it must be better to ask Veronique and Raffaella.
    
    I attached a clas12 raw data (evio format) at https://www.dropbox.com/s/f9683og7fb7jvx7/clas_005038.evio.01102?dl=0
    , which is one of 1239 evio files.
    It is already about 2 GB, so I hope one single data is enough for now.
    
    Now, evio files should be decoded by coatjava. There is one decoder script, evio2hipo at https://github.com/JeffersonLab/clas12-offline-software/blob/development/bin/evio2hipo
    Coatjava documents are relatively well-maintained at https://github.com/JeffersonLab/clas12-offline-software.
    
    Usage: evio2hipo -r 11 -t -1.0 -s -1.0 -i clas_005038.evio.01102 -o decoded.hipo
    
    After decoding, reconstruction should be done using CLARA, and I suspect this is what you’re looking for.
    Unfortunately, I am not that familiar with reconstruction packages.
    There is one reconstructor script, recon-util (formerly notsouseful-util) at https://github.com/JeffersonLab/clas12-offline-software/blob/development/bin/recon-util
    This util will reconstruct events from decoded hipo, using each detector reconstruction modules.
    For example, https://github.com/JeffersonLab/clas12-offline-software/blob/development/common-tools/clas-reco/src/main/java/org/jlab/clas/reco/EngineProcessor.java
    is referred as “org.jlab.clas.reco.EngineProcessor”at recon-util.
    
    Usage: recon-util -i decoded.hipo -o reconstructed.hipo -c 2
    
    So we only need find dc reconstruction modules.
    I guess they are at  https://github.com/JeffersonLab/clas12-offline-software/tree/development/reconstruction/dc, but I might be wrong.
    This is being developed (https://github.com/JeffersonLab/clas12-offline-software/blob/development/reconstruction/dc/README.md)
    COATJAVA README.md (https://github.com/JeffersonLab/clas12-offline-software/blob/development/README.md) says hardcoded dc dependencies are at https://clasweb.jlab.org/clas12maven/.
    
    Please find https://www.jlab.org/indico/event/294/contribution/6/material/slides/7.pdf for overall clas12 software structure if you are confused with terms.
    There are also related repos.
    DC calibration and monitoring: https://github.com/JeffersonLab/clas12dc
    seemingly abandoned dc reconstruction package: https://github.com/JeffersonLab/clara-dc-engine
    
    tape location: /mss/clas12/rg-a/data/
    
    documentation for mass storage system and how to read from it: https://scicomp.jlab.org/docs/storage
    
     
    
    example how to get a file:
    
    jcache get /mss/clas12/rg-a/data/clas_003007.evio.0
    
    wait some time (minutes or hours depending on the speed of tape server and how busy they are)
    
    the file will be moved from tape to hard drive at this location:
    
    /cache/clas12/rg-a/data/clas_003007.evio.0
    
    
\section{Paul Stoler DVMP Cross Section}
    Hi Paul,
    
    I hope this message finds you well. I've been studying for an upcoming exam so have not been able to do much analysis work, but only have about a week left and then am very much looking forward to being able to dig into the data, which very clearly is starting to become quite interesting. 
    
    I'm writing to you with a small question I have about DVMP cross sections. Specifically, there is a certain way that we (the CLAS collaboration, others) tend to write out the 4-fold differential pion electroproduction cross-section - the form written in the purple box in the image below (or equivalently, the equation B1 on page 24 from the 2014 pion cross section paper: https://arxiv.org/pdf/1405.0988.pdf ) 
    
    image.png
    
    It seems to me that this way of writing out the cross section is model-independent, and is just writing out the terms of what contributions you could possibly have to your cross section. From my understanding, the GPDs are encoded as convolutions in the structure functions (e.g. sigma_L  in the green box above, or equivalently equation B20 on page 26 of the linked paper). The exact form of the structure function's dependence on the GPDs is model dependent, for example, there is the Goloskokov Kroll model relating the structure functions to the GPDs. Is my understanding here correct? Or does the GK model (and others) make predictions about the GPDs themselves?
    
    Hope to hear from you if you get a chance, and hope all is as well as reasonably possible
    
    Thanks,
    Bobby
    
    
    Hi Bobby: 
	Here is a brief summary. Let me know if it helps, and please feel free to ask more specific questions.
	In Bedlinsky et al. Eq. B4 is a conventional way of expanding the differential cross section in terms of an effective virtual  photon flux G and a virtual photon cross section for a reaction  gv  + N -> p + N’ , where p is either either p0, p+ or p-.
	The virtual photon cross section consists of the squares of  matrix elements, M(I,f),  of all the possible spin/polarization helicity states of the initial and final configurations i, f.
	The virtual photon cross section is further  broken up into four components, each consisting of grouping of the squares of individual matrix elements,  according to the longitudinal or transverse helicity of the interacting photon.  	 
	These matrix elements are composed of the convolutions of the elementary reaction and the distribution of the partons in the nucleon, which are encoded by the GPDs, according to the relative helicities of the initial and final nucleons as a whole.
    These convolutions involve models of the “elementary” reactions and the GPDs. 
    	In the calculations of GK, the model for the elementary reaction is assumed given, and  the model of the GPDs is written in terms various parameters, which they have constrained based on the scant existing data at that time.
    	In Bedlinsky’s article, which is based on his thesis,  the comparison is shown for the latest CLAS6 data and the original GK model. The result does appear to verify  that there is indeed a dominance of transversity photon quark helicity flip GPDs in this reaction. 
    	But that is just the beginning. These parameters are then adjusted such that the four calculated  structure functions give the closest fit to the available data. 
    	However, one further caveat. The convolution of the GPDs with the elementary reaction within the proton includes things like the Sudakov effect and the so called chiral condensate factor  mm    , which is different for each meson, which multiplies the whole thing and, and which I gather  is by no means precisely determined.
    	So, there is my brief summary. You can get a better idea of the details of what is going onin GK  from Regali’s thesis. Regali rewrote the GK process in C++ for running which experimeters can run themselves.It is large, but  I can send you copy.
    	Does this make any sense? Please free to keep the conversation going, and let’s include other p electroproduction folks in the group.
    
    Regards,
    Paul
    
    
    
    


\section{Convo wiwth Andrey in beginning of 2020}
Trains:
/work/clas12/rg-a/trains/v16_v2/skim8_ep

Hi Andrey,

In test.groovy, line 15 has "MyMods.enable()". What does this do? When I try to run, I get the error "No such property: MyMods for class: test"

I've tried commenting out this line, but run into errors finding org.jlab modules, which I suspect might be a different issue. 

Hi Bobby,
put $/work/clas12/kenjo/magic.jar and /work/clas12/kenjo/uconn.jar into \$HOME/.groovy/lib directory$ and it should work.
Best,
Andrey.

Another small question - I am trying to calculate Bjorken X and need to take the scalar product of two lorentz vectors - do you know how to do this in groovy / groot / clas modules? I tried doing vector1*vector2 but I got an error. I am having trouble finding documentation for this. I think I am specifically asking for scalar product of two objects from org.jlab.clas.physics.LorentzVector. If you could tell me where I could find documentation for this, that would be great

I have more to share but will do so after I build code up a little more


there is no documentation as far as I know. but you can take a look at source code:
1) https://github.com/JeffersonLab/clas12-offline-software/blob/development/common-tools/clas-physics/src/main/java/org/jlab/clas/physics/LorentzVector.java
2) https://github.com/JeffersonLab/clas12-offline-software/blob/development/common-tools/clas-physics/src/main/java/org/jlab/clas/physics/Vector3.java

What you need is probably lorvect1.vect().dot(lorvect2.vect())



Hi Andrey,

Thanks very much for this, your suggestion worked. I'm attaching here a 2D histogram of x_B and Q^2 that contains all 64 files from /work/clas12/rg-a/trains/v16_v2/skim8_ep. I  wrote my own code based off of what you shared with me, so not all the cuts are implemented and its possible they are implemented incorrectly, but I wanted to get your thoughts on if the histogram looked reasonable? When no cuts are applied to the data, the hottest part of the histogram is centered at much lower x_B, around 0.05, instead of here at about 0.4, which I think makes sense to me. But am otherwise hoping to get your thoughts; although I'm not sure there is much that can be said from just this plot.

Additionally, do you have suggestions on why events report having x_B larger than 1? I suppose issues with reconstruction will lead to this, e.g. an overly-high Q^2 and low p_2 would lead to the unphysical x_B > 1 behavior. 

Finally, I processed 135 Million events, and ~ 100,000 remained after the cuts. Do you have an expectation on how many DVMP events you observe? I'm not sure if 0.1% is a reasonable number (within an order of magnitude)

Hi Andrey,

Here is a small update from last time we've talked. I worked off mostly the structure of your original eppi0 code you sent me and have been able to produce a few interesting plots. Particularly, could you please look at slides 16 and 17, which plot the angle between the lepton and hadron planes, and compare those plots to the x_B and Q^2 distribution histograms for Deeply Virtual Pion Production events? Specifically what I'm asking, is that I have read that for DVMP events, we need Q^2 to be greater than 1 to be in the right kinematic regime. This is not implemented as a hard cut in my code (it was not in the eppi0 code you sent me to begin with either, from what I can tell) but looking at the distribution on slide 12, it seems to work out that way anyways that we get basically no DVMP events with Q^2 < 1 GeV. 

For some reason, when I ran my analysis code on the skims from the most recent cook (described on slide 2) the Q^2 distribution shifts significantly, and most DVMP candidate events have Q^2 less than 1 GeV^2. I would assume that this means I should hard-code in a Q^2 > 1 cut in my groovy script, but, if you compare slides 16 and 17, it seems the new cook reproduces the Phi distribution better than the RGA DNP cook, despite many events having Q^2 < 1. 

Could you give me your thoughts on this? Hope all else is going well for you.


Hi Bobby, so sorry for late reply I was busy with readiness review preparation.
So Q2>1 is indeed for deeply virtual events, however it has no relation with lepton/hadron angle. There are pi0 events in the region below 1GeV2, and they are also pi0 events. The limit on 1GeV2 is somewhat artificial. Ideally we are looking at the asymptotic freedom, so Q2 should be infinity, but we are hoping that 1GeV2 is big enough to apply models that are based on asymptotic freedom. There are many terms also that are proportional to powers of t/Q2. So we need reasonably big Q2 to apply GPDs models. And in fact CLAS kinematics is often questioned to be too small for GPDs theoretical models.
Anyway, inbending CLAS12 data by default don't have any low Q2 data because of the electron acceptance. If you see lower Q2 they might come from outbending data, and then indeed you would need to implement hardcoded cut on Q2. But if it's the same runs, just different cooks, then it's strange, and we should investigate it further.



Hi Andrey,

Thanks for getting back to me, no worries about the delay in getting back, I know you are quite busy.

What you've said makes sense. The different plots are from different cooks, but are also somewhat different runs, and the difference might be due the inbending/outbending as you mentioned. It would be helpful to discuss this more on Thursday on the DVMP meeting, if you are able to call into that.

HI Bobby,
you can always check run information here: https://clas12mon.jlab.org/rga/runs/table/ or here: https://clasweb.jlab.org/rcdb


\section{Including Luminosity in caclucations}

    Hi Andrey,
    
    I'm wondering if you can help me understand how to include luminosity in my clas12 analysis code. I am wondering about the general structure as well as how to specifically implement it in groovy:
    
    I think the code logic is as follows:
    - For each run, retrieve a measure of how much beam passed through the target, I believe in the case of CLAS12 using the Faraday cup to measure beam charge
    - sum the beam charge over all runs being considered and include any relevant corrections factors
    - multiply this by target length, density, etc. to get the integrated luminosity
    - use this value to calculate cross sections.
    
    Is this the correct logic flow?
    
    
    About actually implementing it, there is a variable in REC::Event called beamCharge. I'm a bit confused if this what I'm after, I have seen in the CLAS6 analysis note that there is something about using the gated beam charge vs. ungated beam charge? Do you maybe know of a short example groovy script showing how to correctly retrieve and use the beam charge for luminosity?
    
    
    Please let me know when you can. I've copied Sangbaek and Patrick on this message as well - please add to this question if desired.
    
    Thanks,
    Bobby
    
    Hi Bobby,
    you are correct with the logic and I think beamCharge from REC::Event is the proper variable to look at. I think it's already gated which is what you are after. I'll check with Nathan and Nick because we had mix up with gated/ungated and it was supposed to be fixed soon, and let you know.
    Best,
    Andrey.
    
    
    Hi Sangbaek and Patrick,



    Have either of you begun including luminosity in your analysis, to move from counting events to getting a cross section measure out?
    
    
    
    I'm specifically wondering how to implement it in the groovy / java code. I of course understand the general principle of luminosity but am not sure how to handle a few details about it, and also not sure if there is an easy way in the codebase to access the right variables.
    
    
    
    All code Andrey has shared with me hasn't had any of this information in it. Have you any experience with this?
        
        
    Hey Bobby,

    This is a very good question and one that I have wondered too. All I can offer is some speculation. The REC::Event bank contains a variable called beamCharge. Is this the charge in the Faraday cup? Is so, can we use this to calculate the integrated luminosity? We should definitely get confirmation on this.
    
    Best,
    Pat
    
    
    Yes, beamCharge is the beam charge collected from Faraday cup according to https://clasweb.jlab.org/wiki/index.php/CLAS12_DSTs  and https://clas12.discourse.group/t/accessing-beam-charge-information/239
    But I guess it's correct to use the gated charge but I'm not sure.
    Also I don't know if scalers and events are recorded in the same time window.  Maybe we should ask questions regarding luminosity to Andrey who manages charge monitoring at https://clas12mon.jlab.org/charge/?
    
    Best,
    Sangbaek
        
    
\section{How to run locally instead of on ifarm}

    When I work on analysis of CLAS12 data, I've been working exclusively off of ifarm. This has been OK, but not great as I prefer to use my custom text editor and not a command line one like vim or emacs, and also ifarm can sometimes run very slowly.
    
    Do you also work only on ifarm to analyze data, or have you found another method? The data files are probably too large for it to be reasonable to copy all to a local location, but I'm wondering if you are aware of doing any of the these things:
    
    1 - copying a small sample of the data (e.g. only one hipo file) to local and working on it from there for testing, and then just running on ifarm to get more statistics once the code is fleshed out. This I think is the easiest technically, but I'll have to go through setting up the right versions of Root and other software. I guess maybe there would be a Docker container that could be used for this?
    
    2 - faster version of ifarm - I'm not sure if there is any other computing resources jlab has where you can submit a set of instructions and have the results returned to you a few hours later, similar to htcondor but specific to jlab users. This wouldn't get much besides a speed increase on the large datasets I guess, and I can't imagine it would be better than a factor of 2 if anything. Probably would need like an order of magnitude increase in speed to make it worthwhile. I still haven't implemented the multithreading that Sangbaek sent around a while ago, does this significantly decrease processing time?
    
    
    What do you guys think? I guess I'm really interested in the best way to accomplish (1), being able to work totally off ifarm for testing purposes would be really helpful. I think this probably got covered when we were first getting into CLAS12 but its been so long I don't remember where to look. I guess looking at the CLAS12 offline somtware?

    Actually I use both. I usually work on codes in my local and test on one data file if they're working, mainly because I prefer to use my own text editor like you. Then, I use github to apply the changes in ifarm.
    
    . I think this probably got covered when we were first getting into CLAS12 but its been so long I don't remember where to look. I guess looking at the CLAS12 offline somtware?
    As for 1, to test in my local, I needed to download coatjava at https://github.com/JeffersonLab/clas12-offline-software/releases/tag/6.5.8 . Simply unzip tar.gz file and add bin directory in your path. e.g.)
    
    export COATJAVA=/your/coatjava/location
    export PATH=$COATJAVA/bin:$PATH
    
    You don't have to get exactly the same versions of ROOT and coatjava because you don't cook the data by yourself. Any coatjava supporting hipo4 is fine.
    
     I still haven't implemented the multithreading that Sangbaek sent around a while ago, does this significantly decrease processing time?
    My answer to this question is yes, naively by 6 times, because it uses 6 cores. But it may be slightly slower than the ideal case. But still, there are so many advantages of this code. It keeps saving whenever each core processes a little portion. So even if you stop the code running, you still see the intermediate results, which helps you debugging. Also you can run many scripts at one time. If you have multiple versions of your analysis codes, just add them on the main file. Remeber to change run-groovy's memory limit to 4096 from 2048!
    
    2 - faster version of ifarm
    I sometimes use swif to launch jobs at the batch farm. Swif, a.k.a. workflow is nothing but JLab's tool to help launch jobs to its slurm farm. Shooting jobs to swif takes too long because jlab computing resource is too busy to be used for rg-b cooking and GlueX now. But it allows me to turn my laptop off during my code is running. Let me know if you are interested in running a sample swif jobs.
    
    Also if you want to have smaller but complete data files in your local, consider using FX's files.
    
    Bobby, FX has eppi0 train data, which can significantly reduce code run time at /work/clas12/fxgirod/eppi0/
    I guess there’s train for Patricks’ phi too at /work/clas12/fxgirod/epKpKm/, but I’m not sure.
    
    We can have a short meeting to go through these items.
    
    Best,
    Sangbaek
    
    
    
\section{Other}
Forgot to mention that if you want to use this syntactic sugar in your groovy scripts, you can just download jar file and place it in groovy lib directory.
https://github.com/drewkenjo/dst_monitoring/releases



\section{Errors with FX}
I created this fits using what I assume is just a standard ROOT fitter (I'm writing in pyroot):
func = TF1('func', '[0] + [1]*cos(x*pi/180) + [2]*cos(2*x*pi/180)', 0, 360)
 fit = h1.Fit('func', 'QR')

Do you have suggestions, or an example, of how to include errors on my fit? I could look up how to do this online, but first wanted to see if you were referring to a certain method specifically. I don't have much experience with ROOT, so please forgive my lack of knowledge. 

Thanks for the help,
Bobby 

There are two main strategies I can see

If you want to use histograms, ROOT will assume that what you have are counts and set the errors to sqrt(bin content). I believe these are the errors ROOT uses when you perform a fit on a histogram by default. You can display the values of the errors by doing histogram->Draw(“E”) option. There are other options, if you want to force the histogram look you can do “HE” option or if you want to draw the histogram with points and error the option would be “PE” I think. We can have a brief chat over bluejeans to debug this out interactively if you want

The other strategy is if you want to use TGraphErrors objects. The TGraphErrors are initialed with a number of points, and either tables of values, or pointers to the arrays of values. I can also provide simple examples for that if you want. Here you can specify both horizontal and vertical uncertainties.

Please let me know if you want a simple macro example or to talk it out over bluejeans

Best regards

If you have a simple macro example, I think it would be useful for me to look at it and try to implement it; if there are difficulties we could then discuss over bluejeans.

Regarding the two strategies; is there a preferred option or are these two functionally equivalent


The main differences I can think of right now between using a histogram and a TGraphErrors:
- histograms normally do not have negative bin contents, although ROOT usually handle this without problem
- when using a histogram type you need to make sure you set the errors correctly. For instance there are automatic methods to divide one histogram by another, but these do not know how to set errors in the general case
- when performing a fit, you need both horizontal and vertical information, values and errors. You can set horizontal errors to a constant in a histogram, but I am not certain you can have variable horizontal errors. In addition, I believe the horizontal value used in the fit is the center of the bin

Because of these difference I generally prefer to use TGraph types, at least I know exactly what the points are

Please find an example macro attached
This is meant to be interpreted
Please let me know if you have any problem with that, I can edit it to improve the quality if you need to compile it




\section{Random Notes}
U channel needs central detector

Hi Colleagues: Bobby, Andrey,  et al.:
Getting back to Valery’s initial comments re. the very interesting graphs which were shown for DVMP (pi0) events. They  showed that a large number of  recoil protons are detected in the central detector - even more than in the forward detector. Of course for the transversely GPD calculations, the t dependence is crucial.  Is it possible to show the same type graphs, but as a function of the t variable. Also, even more interesting would be, a two dimensional graphs  showing the distributions of events vs Q2 and t  for a given W, e.g. 3 GeV, for both the forward and central detectors.
Best regards,
Paul Stoler

Thanks for the table.  It is very useful.  I think that if we have plots for t-distribution for q2 vs. x_b bins for different exclusive channels, which are similar to what Bobby showed during the last meeting, things will be more clear.      I think that  t-distribution for each q2 vs. x_b will clearly shows where we are losing.  These plots are similar to what Paul asked the other day but I think this would be good enough since we are analyzing data in q2, x_b, and t as you pointed out.   Thanks.

best regards, Kyungseon

I agree that we still need to look at the phi coverage to see whether that bin is still analyzable.  However, I think that at this point, plots for t-distributions for a given q2 vs. x_b bin will give us enough idea of our data landscape for different channels.  Thanks.


Dear all,

I highly appreciate if everyone who does exclusive channel analysis, makes plots for t-distribution for q2 vs. x_b bins as Bobby showed today with/without central tracking (or forwarding and central tracking separately depending on your channel) to see the impact of central tracking in terms of kinematic coverage.  It would be very useful to know for the collaboration.   Thanks.

Best regards, Kyungseon


Dear All,

I would like to stress the importance of the central detector in the analysis of the exclusive reactions, DVCS and DVMP. We saw today that the number of the DVMP events are at least twice  more when  proton is detected in the central detector.  But it is not only the statistics that will be double with the inclusion of the central detector. These events have small t, and it is extremely important for the GPD physics. The parameter t/Q2 controls the contribution of  the high twists. So, the central detector will supply the most valuable data set from the physics point of view. When we try to extract the GPD parameters from the experimental data (that we are doing right now) the theorists always request to include data only with t/Q2<1, or even t/Q2<<1. This condition is not always satisfied in the CLAS6 kinematics. That is why we want to have fully operational central detector. I hope that we will make another pass1(or pass2 if you want) this year with improved central tracking that will give us the possibility to explore full CLAS12 kinematics.



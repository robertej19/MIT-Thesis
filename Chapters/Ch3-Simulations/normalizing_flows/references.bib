@article{PhysRevD.55.7114,
  title = {Deeply Virtual Compton Scattering},
  author = {X.~Ji},
  journal = {Phys.~Rev.~D},
  volume = {55},
  issue = {11},
  pages = {7114--7125},
  numpages = {0},
  year = {1997},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.55.7114},
}

@article{BURKERT2020163419,
title = {The CLAS12 Spectrometer at Jefferson Laboratory},
journal = {Nucl.~Inst.~and Meth.~A},
volume = {959},
pages = {163419},
year = {2020},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2020.163419},
author = {V.~D.~Burkert and others},
keywords = {CLAS12, Magnetic spectrometer, Electromagnetic physics, Large acceptance, Luminosity},
}

@article{10.1093/ptep/ptaa104,
    author = {P.~A.~Zyla and others},
    title = "{Review of Particle Physics}",
    journal = {Progress of Theoretical and Experimental Physics},
    volume = {2020},
    number = {8},
    year = {2020},
    abstract = "{The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 3,324 new measurements from 878 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as supersymmetric particles, heavy bosons, axions, dark photons, etc. Particle properties and search limits are listed in Summary Tables. We give numerous tables, figures, formulae, and reviews of topics such as Higgs Boson Physics, Supersymmetry, Grand Unified Theories, Neutrino Mixing, Dark Energy, Dark Matter, Cosmology, Particle Detectors, Colliders, Probability and Statistics. Among the 120 reviews are many that are new or heavily revised, including a new review on High Energy Soft QCD and Diffraction and one on the Determination of CKM Angles from B Hadrons.The Review is divided into two volumes. Volume 1 includes the Summary Tables and 98 review articles. Volume 2 consists of the Particle Listings and contains also 22 reviews that address specific aspects of the data presented in the Listings.The complete Review (both volumes) is published online on the website of the Particle Data Group (pdg.lbl.gov) and in a journal. Volume 1 is available in print as the PDG Book. A Particle Physics Booklet with the Summary Tables and essential tables, figures, and equations from selected review articles is available in print and as a web version optimized for use on phones as well as an Android app.}",
    issn = {2050-3911},
    doi = {10.1093/ptep/ptaa104},
    note = {083C01},
    eprint = {https://academic.oup.com/ptep/article-pdf/2020/8/083C01/34673722/ptaa104.pdf},
}




@article{PhysRevLett.115.212003,
  title = {Cross Sections for the Exclusive Photon Electroproduction on the Proton and Generalized Parton Distributions},
  author = {H.~S.~Jo and others},
  collaboration = {CLAS Collaboration},
  journal = {Phys.~Rev.~Lett.},
  volume = {115},
  issue = {21},
  pages = {212003},
  numpages = {7},
  year = {2015},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.115.212003},
}

%url = {https://cds.cern.ch/record/2707778/files/CERN-THESIS-2020-003.pdf}
@mastersthesis{Viljoen2020,
    author = {C.~G.~Viljoen},
    title = {Machine Learning for Particle Identification \& Deep Generative Models Towards Fast Simulations For The ALICE Transistion Radiation Detector at CERN},
    school = {University of Cape Town},
    year = {2020}
}

%https://arxiv.org/abs/1807.02876
@article{Albertsson2018,
title = {Machine Learning in High Energy Physics Community White Paper},
journal = {Journal of Physics: Conference Series},
volume = {1085},
year = {2018},
issn = {022008},
author = {K.~Albertsson and others},
}

%https://arxiv.org/pdf/1705.02355.pdf
@article{Paganini2017,
title = {Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorimeters},
author = {M.~Paganini and others},
journal = {ArXiv},
year = {2017},
}

%How can this paper exist in one of top rows?
@inproceedings{wehenkel2019unconstrained,
  title={Unconstrained monotonic neural networks},
  author={A.~Wehenkel and G.~Louppe},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1543--1553},
  year={2019}
}



@article{AGOSTINELLI2003250,
title = {Geant4—a simulation toolkit},
journal = {Nucl.~Inst.~and Meth.~A},
volume = {506},
number = {3},
pages = {250-303},
year = {2003},
issn = {0168-9002},
doi = {https://doi.org/10.1016/S0168-9002(03)01368-8},
author = {S.~Agostinelli and others},
keywords = {Simulation, Particle interactions, Geometrical modelling, Software engineering, Object-oriented technology, Distributed software development},
abstract = {Geant4 is a toolkit for simulating the passage of particles through matter. It includes a complete range of functionality including tracking, geometry, physics models and hits. The physics processes offered cover a comprehensive range, including electromagnetic, hadronic and optical processes, a large set of long-lived particles, materials and elements, over a wide energy range starting, in some cases, from 250eV and extending in others to the TeV energy range. It has been designed and constructed to expose the physics models utilised, to handle complex geometries, and to enable its easy adaptation for optimal use in different sets of applications. The toolkit is the result of a worldwide collaboration of physicists and software engineers. It has been created exploiting software engineering and object-oriented technology and implemented in the C++ programming language. It has been used in applications in particle physics, nuclear physics, accelerator design, space engineering and medical physics.}
}



@article{PhysRevD.101.076002,
  title = {Event generation with normalizing flows},
  author = {C.~Gao and others},
  journal = {Phys. Rev. D},
  volume = {101},
  issue = {7},
  pages = {076002},
  numpages = {8},
  year = {2020},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.101.076002},
}

@ARTICLE{9089305,
  author={I.~{Kobyzev} and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Normalizing Flows: An Introduction and Review of Current Methods}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2020.2992934}}
  
  @misc{papamakarios2019normalizing,
      title={Normalizing Flows for Probabilistic Modeling and Inference}, 
      author={G.~Papamakarios and others},
      year={2019},
      eprint={1912.02762},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Dinh15,
  author    = {L.~Dinh and others},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year      = {2015},
}

@article{root,
title = {ROOT — A C++ framework for petabyte data storage, statistical analysis and visualization},
journal = {Computer Physics Communications},
volume = {180},
number = {12},
pages = {2499-2512},
year = {2009},
doi = {https://doi.org/10.1016/j.cpc.2009.08.005},
author = {I.~Antcheva and others},
keywords = {C++, Object-oriented, Framework, Interpreter, Data storage, Data analysis, Visualization},
abstract = {ROOT is an object-oriented C++ framework conceived in the high-energy physics (HEP) community, designed for storing and analyzing petabytes of data in an efficient way. Any instance of a C++ class can be stored into a ROOT file in a machine-independent compressed binary format. In ROOT the TTree object container is optimized for statistical data analysis over very large data sets by using vertical data storage techniques. These containers can span a large number of files on local disks, the web, or a number of different shared file systems. In order to analyze this data, the user can chose out of a wide set of mathematical and statistical functions, including linear algebra classes, numerical algorithms such as integration and minimization, and various methods for performing regression analysis (fitting). In particular, the RooFit package allows the user to perform complex data modeling and fitting while the RooStats library provides abstractions and implementations for advanced statistical tools. Multivariate classification methods based on machine learning techniques are available via the TMVA package. A central piece in these analysis tools are the histogram classes which provide binning of one- and multi-dimensional data. Results can be saved in high-quality graphical formats like Postscript and PDF or in bitmap formats like JPG or GIF. The result can also be stored into ROOT macros that allow a full recreation and rework of the graphics. Users typically create their analysis macros step by step, making use of the interactive C++ interpreter CINT, while running over small data samples. Once the development is finished, they can run these macros at full compiled speed over large data sets, using on-the-fly compilation, or by creating a stand-alone batch program. Finally, if processing farms are available, the user can reduce the execution time of intrinsically parallel tasks — e.g. data mining in HEP — by using PROOF, which will take care of optimally distributing the work over the available resources in a transparent way.
Program summary
Program title: ROOT Catalogue identifier: AEFA_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEFA_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: LGPL No. of lines in distributed program, including test data, etc.: 3 044 581 No. of bytes in distributed program, including test data, etc.: 36 325 133 Distribution format: tar.gz Programming language: C++ Computer: Intel i386, Intel x86-64, Motorola PPC, Sun Sparc, HP PA-RISC Operating system: GNU/Linux, Windows XP/Vista, Mac OS X, FreeBSD, OpenBSD, Solaris, HP-UX, AIX Has the code been vectorized or parallelized?: Yes RAM: >55 Mbytes Classification: 4, 9, 11.9, 14 Nature of problem: Storage, analysis and visualization of scientific data Solution method: Object store, wide range of analysis algorithms and visualization methods Additional comments: For an up-to-date author list see: http://root.cern.ch/drupal/content/root-development-team and http://root.cern.ch/drupal/content/former-root-developers Running time: Depending on the data size and complexity of analysis algorithms References:[1]http://root.cern.ch.}
}


@article{ uproot,
	author = {{E.~Rodrigues} and others},
	title = {The Scikit HEP Project overview and prospects},
	DOI= "10.1051/epjconf/202024506028",
	journal = {EPJ Web Conf.},
	year = 2020,
	volume = 245,
	pages = "06028",
}



@article{phialia,
  title = {Equivariant Flow-Based Sampling for Lattice Gauge Theory},
  author = {G.~Kanwar and others},
  journal = {Phys. Rev. Lett.},
  volume = {125},
  issue = {12},
  pages = {121601},
  numpages = {6},
  year = {2020},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.125.121601},
}


@misc{ stan,
    author = {C.~Weisser},
    year = {2021},
    title = {The Search for Dark Photons at LHCb and Machine Learning in Particle Physics, Ph.~D.~Thesis (ongoing)}
}

@misc{nflows,
  author       = {C.~Durkan and others},
  title        = {{nflows}: normalizing flows in {PyTorch}},
  month        = nov,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.14},
  doi          = {10.5281/zenodo.4296287},
  url          = {https://doi.org/10.5281/zenodo.4296287}
}

@misc{papamakarios2018masked,
      title={Masked Autoregressive Flow for Density Estimation}, 
      author={G.~Papamakarios and others},
      year={2018},
      eprint={1705.07057},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Kullback51klDivergence,
  added-at = {2010-10-31T19:59:47.000+0100},
  author = {S.~Kullback and R.~A.~Leibler},
  biburl = {https://www.bibsonomy.org/bibtex/2560a5719c537c5c4a496bfebd4a21603/lee_peck},
  description = {Kullback , Leibler : On Information and Sufficiency},
  interhash = {f9d41d76a07383cca4c3a1a94c24d533},
  intrahash = {560a5719c537c5c4a496bfebd4a21603},
  journal = {Ann.~Math.~Statist.},
  keywords = {51 Kullback Leibler divergence kl},
  number = 1,
  pages = {79-86},
  timestamp = {2010-10-31T19:59:47.000+0100},
  title = {On Information and Sufficiency},
  volume = 22,
  year = 1951
}

@article{Dobrushin,
author = {R.~L.~Dobrushin},
title = {Prescribing a System of Random Variables by Conditional Distributions},
journal = {Theory of Probability \& Its Applications},
volume = {15},
number = {3},
pages = {458-486},
year = {1970},
doi = {10.1137/1115049},
}

@ARTICLE{jsd,
  author={J.~Lin},
  journal={IEEE Transactions on Information Theory}, 
  title={Divergence measures based on the Shannon entropy}, 
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  doi={10.1109/18.61115}}
  
@inproceedings{NEURIPS20192a084e55,
 author = {A.~Wehenkel and G.~Louppe},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unconstrained Monotonic Neural Networks},
 volume = {32},
 year = {2019}
}


@InProceedings{pmlr-v37-germain15,
title = {MADE: Masked Autoencoder for Distribution Estimation},
author = {M.~Germain and others},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {881--889}, year = {2015},
volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR} }

@inproceedings{NIPS2016ddeebdee,
 author = {D.~P.~Kingma and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 volume = {29},
 year = {2016}
}



@article {Belkin15849,
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	title = {Reconciling modern machine-learning practice and the classical bias{\textendash}variance trade-off},
	volume = {116},
	number = {32},
	pages = {15849--15854},
	year = {2019},
	doi = {10.1073/pnas.1903070116},
	publisher = {National Academy of Sciences},
	abstract = {While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms.Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias{\textendash}variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias{\textendash}variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This {\textquotedblleft}double-descent{\textquotedblright} curve subsumes the textbook U-shaped bias{\textendash}variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	issn = {0027-8424},
	eprint = {https://www.pnas.org/content/116/32/15849.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article {Radhakrishnan27162,
	author = {Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
	title = {Overparameterized neural networks implement associative memory},
	volume = {117},
	number = {44},
	pages = {27162--27170},
	year = {2020},
	doi = {10.1073/pnas.2005013117},
	publisher = {National Academy of Sciences},
	abstract = {Development of computational models of memory is a subject of long-standing interest at the intersection of machine learning and neuroscience. Our main finding is that overparameterized neural networks trained using standard optimization methods provide a simple mechanism for implementing associative memory. Remarkably, this mechanism allows for the storage and retrieval of sequences of examples. This finding also sheds light on inductive biases in overparameterized networks: while there are many functions that can achieve zero training loss in the overparameterized regime, our result shows that increasing depth and width in neural networks leads to maps that are more contractive around training examples, thereby allowing for storage and retrieval of more training examples.Identifying computational mechanisms for memorization and retrieval of data is a long-standing problem at the intersection of machine learning and neuroscience. Our main finding is that standard overparameterized deep neural networks trained using standard optimization methods implement such a mechanism for real-valued data. We provide empirical evidence that 1) overparameterized autoencoders store training samples as attractors and thus iterating the learned map leads to sample recovery, and that 2) the same mechanism allows for encoding sequences of examples and serves as an even more efficient mechanism for memory than autoencoding. Theoretically, we prove that when trained on a single example, autoencoders store the example as an attractor. Lastly, by treating a sequence encoder as a composition of maps, we prove that sequence encoding provides a more efficient mechanism for memory than autoencoding.All study data are included in the article and SI Appendix.},
	issn = {0027-8424},
	eprint = {https://www.pnas.org/content/117/44/27162.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

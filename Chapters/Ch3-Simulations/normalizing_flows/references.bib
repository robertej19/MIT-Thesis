
%url = {https://cds.cern.ch/record/2707778/files/CERN-THESIS-2020-003.pdf}
@mastersthesis{Viljoen2020,
    author = {C.~G.~Viljoen},
    title = {Machine Learning for Particle Identification \& Deep Generative Models Towards Fast Simulations For The ALICE Transistion Radiation Detector at CERN},
    school = {University of Cape Town},
    year = {2020}
}

%https://arxiv.org/abs/1807.02876
@article{Albertsson2018,
title = {Machine Learning in High Energy Physics Community White Paper},
journal = {Journal of Physics: Conference Series},
volume = {1085},
year = {2018},
issn = {022008},
author = {K.~Albertsson and others},
}

%https://arxiv.org/pdf/1705.02355.pdf
@article{Paganini2017,
title = {Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorimeters},
author = {M.~Paganini and others},
journal = {ArXiv},
year = {2017},
}

%How can this paper exist in one of top rows?
@inproceedings{wehenkel2019unconstrained,
  title={Unconstrained monotonic neural networks},
  author={A.~Wehenkel and G.~Louppe},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1543--1553},
  year={2019}
}



@article{AGOSTINELLI2003250,
title = {Geant4â€”a simulation toolkit},
journal = {Nucl.~Inst.~and Meth.~A},
volume = {506},
number = {3},
pages = {250-303},
year = {2003},
issn = {0168-9002},
doi = {https://doi.org/10.1016/S0168-9002(03)01368-8},
author = {S.~Agostinelli and others},
keywords = {Simulation, Particle interactions, Geometrical modelling, Software engineering, Object-oriented technology, Distributed software development},
abstract = {Geant4 is a toolkit for simulating the passage of particles through matter. It includes a complete range of functionality including tracking, geometry, physics models and hits. The physics processes offered cover a comprehensive range, including electromagnetic, hadronic and optical processes, a large set of long-lived particles, materials and elements, over a wide energy range starting, in some cases, from 250eV and extending in others to the TeV energy range. It has been designed and constructed to expose the physics models utilised, to handle complex geometries, and to enable its easy adaptation for optimal use in different sets of applications. The toolkit is the result of a worldwide collaboration of physicists and software engineers. It has been created exploiting software engineering and object-oriented technology and implemented in the C++ programming language. It has been used in applications in particle physics, nuclear physics, accelerator design, space engineering and medical physics.}
}



@article{PhysRevD.101.076002,
  title = {Event generation with normalizing flows},
  author = {C.~Gao and others},
  journal = {Phys. Rev. D},
  volume = {101},
  issue = {7},
  pages = {076002},
  numpages = {8},
  year = {2020},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.101.076002},
}

  
  @misc{papamakarios2019normalizing,
      title={Normalizing Flows for Probabilistic Modeling and Inference}, 
      author={G.~Papamakarios and others},
      year={2019},
      eprint={1912.02762},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}




@misc{papamakarios2018masked,
      title={Masked Autoregressive Flow for Density Estimation}, 
      author={G.~Papamakarios and others},
      year={2018},
      eprint={1705.07057},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Kullback51klDivergence,
  added-at = {2010-10-31T19:59:47.000+0100},
  author = {S.~Kullback and R.~A.~Leibler},
  biburl = {https://www.bibsonomy.org/bibtex/2560a5719c537c5c4a496bfebd4a21603/lee_peck},
  description = {Kullback , Leibler : On Information and Sufficiency},
  interhash = {f9d41d76a07383cca4c3a1a94c24d533},
  intrahash = {560a5719c537c5c4a496bfebd4a21603},
  journal = {Ann.~Math.~Statist.},
  keywords = {51 Kullback Leibler divergence kl},
  number = 1,
  pages = {79-86},
  timestamp = {2010-10-31T19:59:47.000+0100},
  title = {On Information and Sufficiency},
  volume = 22,
  year = 1951
}



@ARTICLE{jsd,
  author={J.~Lin},
  journal={IEEE Transactions on Information Theory}, 
  title={Divergence measures based on the Shannon entropy}, 
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  doi={10.1109/18.61115}}
  

@InProceedings{pmlr-v37-germain15,
title = {MADE: Masked Autoencoder for Distribution Estimation},
author = {M.~Germain and others},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {881--889}, year = {2015},
volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR} }

@inproceedings{NIPS2016ddeebdee,
 author = {D.~P.~Kingma and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 volume = {29},
 year = {2016}
}



@article {Belkin15849,
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	title = {Reconciling modern machine-learning practice and the classical bias{\textendash}variance trade-off},
	volume = {116},
	number = {32},
	pages = {15849--15854},
	year = {2019},
	doi = {10.1073/pnas.1903070116},
	publisher = {National Academy of Sciences},
	abstract = {While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms.Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias{\textendash}variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias{\textendash}variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This {\textquotedblleft}double-descent{\textquotedblright} curve subsumes the textbook U-shaped bias{\textendash}variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	issn = {0027-8424},
	eprint = {https://www.pnas.org/content/116/32/15849.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article {Radhakrishnan27162,
	author = {Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
	title = {Overparameterized neural networks implement associative memory},
	volume = {117},
	number = {44},
	pages = {27162--27170},
	year = {2020},
	doi = {10.1073/pnas.2005013117},
	publisher = {National Academy of Sciences},
	abstract = {Development of computational models of memory is a subject of long-standing interest at the intersection of machine learning and neuroscience. Our main finding is that overparameterized neural networks trained using standard optimization methods provide a simple mechanism for implementing associative memory. Remarkably, this mechanism allows for the storage and retrieval of sequences of examples. This finding also sheds light on inductive biases in overparameterized networks: while there are many functions that can achieve zero training loss in the overparameterized regime, our result shows that increasing depth and width in neural networks leads to maps that are more contractive around training examples, thereby allowing for storage and retrieval of more training examples.Identifying computational mechanisms for memorization and retrieval of data is a long-standing problem at the intersection of machine learning and neuroscience. Our main finding is that standard overparameterized deep neural networks trained using standard optimization methods implement such a mechanism for real-valued data. We provide empirical evidence that 1) overparameterized autoencoders store training samples as attractors and thus iterating the learned map leads to sample recovery, and that 2) the same mechanism allows for encoding sequences of examples and serves as an even more efficient mechanism for memory than autoencoding. Theoretically, we prove that when trained on a single example, autoencoders store the example as an attractor. Lastly, by treating a sequence encoder as a composition of maps, we prove that sequence encoding provides a more efficient mechanism for memory than autoencoding.All study data are included in the article and SI Appendix.},
	issn = {0027-8424},
	eprint = {https://www.pnas.org/content/117/44/27162.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
